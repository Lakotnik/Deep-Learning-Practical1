2019-05-16 12:00:25.989284: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2019-05-16 12:00:26.191848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-05-16 12:00:26.192602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GRID V100D-32Q major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:02:00.0
totalMemory: 32.00GiB freeMemory: 29.59GiB
2019-05-16 12:00:26.192661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-05-16 12:00:27.468294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-16 12:00:27.468363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-05-16 12:00:27.468380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-05-16 12:00:27.468550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 28702 MB memory) -> physical GPU (device: 0, name: GRID V100D-32Q, pci bus id: 0000:02:00.0, compute capability: 7.0)
2019-05-16 12:00:27.469447: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Using hard_sigmoid activation function for everything except the last layer.
data shape: (50000, 32, 32, 3)
Saved trained model at /home/p286668/schoolwork/dlearning/Deep-Learning-Practical1/model_save/cifar10_hard_sigmoid.h5 

   32/10000 [..............................] - ETA: 1s
  768/10000 [=>............................] - ETA: 0s
 1536/10000 [===>..........................] - ETA: 0s
 2304/10000 [=====>........................] - ETA: 0s
 3072/10000 [========>.....................] - ETA: 0s
 3840/10000 [==========>...................] - ETA: 0s
 4608/10000 [============>.................] - ETA: 0s
 5376/10000 [===============>..............] - ETA: 0s
 6144/10000 [=================>............] - ETA: 0s
 6912/10000 [===================>..........] - ETA: 0s
 7680/10000 [======================>.......] - ETA: 0s
 8448/10000 [========================>.....] - ETA: 0s
 9216/10000 [==========================>...] - ETA: 0s
 9984/10000 [============================>.] - ETA: 0s
10000/10000 [==============================] - 1s 67us/step
/software/software/h5py/2.7.1-fosscuda-2018a-Python-3.6.4/lib/python3.6/site-packages/h5py-2.7.1-py3.6-linux-x86_64.egg/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
------------------------------------------------
Training took 929.2612750530243 seconds.
Activation:  hard_sigmoid
Dropout:  0.4
Test loss: 0.6383181487083435
Test accuracy: 0.7809
------------------------------------------------


###############################################################################
Peregrine Cluster
Job 5859649 for user 'p286668'
Finished at: Thu May 16 12:16:00 CEST 2019

Job details:
============

Name                : Keras_cifar10
User                : p286668
Partition           : gputest
Nodes               : pg-gpu08
Cores               : 12
State               : COMPLETED
Submit              : 2019-05-16T10:44:42
Start               : 2019-05-16T12:00:18
End                 : 2019-05-16T12:16:00
Reserved walltime   :   01:30:00
Used walltime       :   00:15:42
Used CPU time       : 1-10:32:10 (efficiency: 1099.88%)
% User (Computation): 84.16%
% System (I/O)      : 15.84%
Mem reserved        : 8000M/node
Max Mem used        : 2.86G (pg-gpu08)
Max Disk Write      : 40.96K (pg-gpu08)
Max Disk Read       : 1.19M (pg-gpu08)


Acknowledgements:
=================

Please see this page if you want to acknowledge Peregrine in your publications:

https://redmine.hpc.rug.nl/redmine/projects/peregrine/wiki/ScientificOutput

################################################################################
